{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd04b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3785c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== File: amatol/journals/2019-02-15__sojourn__p45-54__all-aboard-for-amatol-new-jersey.txt ===\n",
      "Content preview: All Aboard for Amatol, NJ\n",
      "As a result of America’s entry into World War I, Atlantic County received a great expansion of its industrial economic base. The largest result of this expansion was the cons …\n",
      "Metadata:\n",
      "  source_type: journal\n",
      "  journal: SoJourn\n",
      "  volume: 3\n",
      "  issue: 2\n",
      "  season: Winter 2018/19\n",
      "  pages: 45-54\n",
      "  title: All Aboard for Amatol, New Jersey\n",
      "  citation: SoJourn 3.2, Winter 2018/19, pp. 45–54, \"All Aboard for Amatol, New Jersey\"\n",
      "  file_path: amatol/journals/2019-02-15__sojourn__p45-54__all-aboard-for-amatol-new-jersey.txt\n"
     ]
    }
   ],
   "source": [
    "# journal_parser.py\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "\n",
    "# --- Load metadata.json once at import ---\n",
    "with open(\"amatol/journals/metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    METADATA = json.load(f)\n",
    "\n",
    "\n",
    "def parse_journal_article(file_path: str) -> Document:\n",
    "    \"\"\"\n",
    "    Parse a journal article into a LangChain Document with metadata.\n",
    "    Expects the filename stem (no .txt) to match keys in metadata.json.\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    fname = path.stem  # no .txt extension\n",
    "\n",
    "    if fname not in METADATA.get(\"journals\", {}):\n",
    "        raise ValueError(f\"File {fname} not found in metadata.json\")\n",
    "\n",
    "    entry = METADATA[\"journals\"][fname]\n",
    "    raw_text = path.read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "    metadata = {\n",
    "        \"source_type\": \"journal\",\n",
    "        \"journal\": entry[\"journal\"],\n",
    "        \"volume\": entry[\"volume\"],\n",
    "        \"issue\": entry[\"issue\"],\n",
    "        \"season\": entry[\"season\"],\n",
    "        \"pages\": entry[\"pages\"],\n",
    "        \"title\": entry[\"title\"],\n",
    "        \"citation\": entry[\"citation\"],\n",
    "        \"file_path\": str(file_path),\n",
    "    }\n",
    "\n",
    "    return Document(page_content=raw_text, metadata=metadata)\n",
    "\n",
    "\n",
    "# --- Demo run ---\n",
    "if __name__ == \"__main__\":\n",
    "    root = Path(\"amatol/journals\")\n",
    "    all_files = root.rglob(\"*.txt\")\n",
    "\n",
    "    for file_path in all_files:\n",
    "        doc = parse_journal_article(file_path)\n",
    "        print(\"\\n=== File:\", file_path, \"===\")\n",
    "        print(\"Content preview:\", doc.page_content[:200], \"…\")\n",
    "        print(\"Metadata:\")\n",
    "        for k, v in doc.metadata.items():\n",
    "            print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538674b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 1 files from ./amatol-test …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing files: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added 3 chunks from journals/2019-02-15__sojourn__p45-54__all-aboard-for-amatol-new-jersey.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm import tqdm\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# --- Settings ---\n",
    "COLLECTION_NAME = 'amatol_docs'\n",
    "ROOT_DIR = './amatol-test'  # Modify this as needed\n",
    "\n",
    "# --- Step 1: Recursively find all .txt files ---\n",
    "def find_txt_files(root_dir: str) -> list[Path]:\n",
    "    return [p for p in Path(root_dir).rglob('*.txt')]\n",
    "\n",
    "# --- Step 2: Load ONE file and attach metadata ---\n",
    "# from journal_parser import parse_journal_article\n",
    "\n",
    "def load_one_document(path: Path, root_dir: str):\n",
    "    if \"journals\" in str(path):  # crude check\n",
    "        return [parse_journal_article(path)]\n",
    "    else:\n",
    "        loader = TextLoader(str(path), encoding=\"utf-8\")\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = str(path.relative_to(root_dir))\n",
    "        return docs\n",
    "\n",
    "# --- Step 3: Chunk helper ---\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "def adaptive_chunk_documents(docs: list[Document], model: str = 'text-embedding-3-small') -> list[Document]:\n",
    "    \"\"\"Take a list of Documents, split adaptively, return list of Documents.\"\"\"\n",
    "    out_docs = []\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        token_count = len(enc.encode(text))\n",
    "\n",
    "        if token_count < 500:\n",
    "            out_docs.append(doc)  # keep whole\n",
    "        elif token_count < 1500:\n",
    "            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                model_name=model, chunk_size=500, chunk_overlap=80\n",
    "            )\n",
    "            out_docs.extend(splitter.split_documents([doc]))\n",
    "        else:\n",
    "            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                model_name=model, chunk_size=800, chunk_overlap=100\n",
    "            )\n",
    "            out_docs.extend(splitter.split_documents([doc]))\n",
    "\n",
    "    return out_docs\n",
    "\n",
    "\n",
    "def _embedding_dim(embeddings) -> int:\n",
    "    return len(embeddings.embed_query('dim?'))\n",
    "\n",
    "def _ensure_collection(client: QdrantClient, name: str, embeddings) -> None:\n",
    "    if not client.collection_exists(name):\n",
    "        dim = _embedding_dim(embeddings)\n",
    "        client.create_collection(\n",
    "            collection_name=name,\n",
    "            vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "# --- Step 4: Process files ONE AT A TIME ---\n",
    "def embed_directory_one_file_at_a_time(root_dir: str, collection_name: str) -> None:\n",
    "    txt_paths = find_txt_files(root_dir)\n",
    "    if not txt_paths:\n",
    "        print(f'No .txt files found under {root_dir}')\n",
    "        return\n",
    "\n",
    "    # Create shared clients once\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    client = QdrantClient(host='localhost', port=6333)\n",
    "    _ensure_collection(client, collection_name, embeddings)\n",
    "    vs = Qdrant(client=client, collection_name=collection_name, embeddings=embeddings)\n",
    "\n",
    "    print(f'Indexing {len(txt_paths)} files from {root_dir} …')\n",
    "    for path in tqdm(txt_paths, desc='Indexing files'):\n",
    "        # 1) load only THIS file\n",
    "        docs = load_one_document(path, root_dir)\n",
    "        # 2) chunk THIS file\n",
    "        chunks = adaptive_chunk_documents(docs)\n",
    "        # 3) upload JUST these chunks\n",
    "        vs.add_documents(chunks)\n",
    "        print(f'  Added {len(chunks)} chunks from {path.relative_to(root_dir)}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embed_directory_one_file_at_a_time(ROOT_DIR, COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0890222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
