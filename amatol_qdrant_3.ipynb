{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd04b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538674b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 2 files from ./data-3 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing files:  50%|█████     | 1/2 [00:00<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added 4 chunks from 1918-11-12, 11, Philadelphia Inquirer, War is over.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing files: 100%|██████████| 2/2 [00:00<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added 1 chunks from 1918-12-05, 4, Philadelphia Inquirer, operations halt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm import tqdm\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# --- Settings ---\n",
    "COLLECTION_NAME = 'amatol_docs'\n",
    "ROOT_DIR = './data-3'  # Modify this as needed\n",
    "\n",
    "# --- Step 1: Recursively find all .txt files ---\n",
    "def find_txt_files(root_dir: str) -> list[Path]:\n",
    "    return [p for p in Path(root_dir).rglob('*.txt')]\n",
    "\n",
    "# --- Step 2: Load ONE file and attach metadata ---\n",
    "def load_one_document(path: Path, root_dir: str) -> list:\n",
    "    loader = TextLoader(str(path), encoding='utf-8')\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        # Keep 'source' for now; we’ll add a deletion-friendly ID in the next step\n",
    "        doc.metadata['source'] = str(path.relative_to(root_dir))\n",
    "    return docs\n",
    "\n",
    "# --- Step 3: Chunk helper ---\n",
    "def chunk_documents(docs: list) -> list:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=550, chunk_overlap=50)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "def _embedding_dim(embeddings) -> int:\n",
    "    return len(embeddings.embed_query('dim?'))\n",
    "\n",
    "def _ensure_collection(client: QdrantClient, name: str, embeddings) -> None:\n",
    "    if not client.collection_exists(name):\n",
    "        dim = _embedding_dim(embeddings)\n",
    "        client.create_collection(\n",
    "            collection_name=name,\n",
    "            vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "# --- Step 4: Process files ONE AT A TIME ---\n",
    "def embed_directory_one_file_at_a_time(root_dir: str, collection_name: str) -> None:\n",
    "    txt_paths = find_txt_files(root_dir)\n",
    "    if not txt_paths:\n",
    "        print(f'No .txt files found under {root_dir}')\n",
    "        return\n",
    "\n",
    "    # Create shared clients once\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    client = QdrantClient(host='localhost', port=6333)\n",
    "    _ensure_collection(client, collection_name, embeddings)\n",
    "    vs = Qdrant(client=client, collection_name=collection_name, embeddings=embeddings)\n",
    "\n",
    "    print(f'Indexing {len(txt_paths)} files from {root_dir} …')\n",
    "    for path in tqdm(txt_paths, desc='Indexing files'):\n",
    "        # 1) load only THIS file\n",
    "        docs = load_one_document(path, root_dir)\n",
    "        # 2) chunk THIS file\n",
    "        chunks = chunk_documents(docs)\n",
    "        # 3) upload JUST these chunks\n",
    "        vs.add_documents(chunks)\n",
    "        print(f'  Added {len(chunks)} chunks from {path.relative_to(root_dir)}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embed_directory_one_file_at_a_time(ROOT_DIR, COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0890222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
