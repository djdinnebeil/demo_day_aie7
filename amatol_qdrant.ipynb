{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm import tqdm\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# --- Settings ---\n",
    "COLLECTION_NAME = \"amatol_docs\"\n",
    "ROOT_DIR = \"./data-test-2\" # Modify this as needed\n",
    "\n",
    "# --- Step 1: Recursively find all .txt files ---\n",
    "def find_txt_files(root_dir: str) -> list[Path]:\n",
    "    return [p for p in Path(root_dir).rglob(\"*.txt\")]\n",
    "\n",
    "# --- Step 2: Load files and attach metadata ---\n",
    "def load_documents(paths: list[Path], root_dir: str) -> list:\n",
    "    all_docs = []\n",
    "    for path in tqdm(paths, desc=\"Loading files\"):\n",
    "        loader = TextLoader(str(path), encoding=\"utf-8\")\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = str(path.relative_to(root_dir))\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs\n",
    "\n",
    "# --- Step 3: Set Chunk Size ---\n",
    "def chunk_documents(docs: list) -> list:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=550, chunk_overlap=50)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "def _embedding_dim(embeddings) -> int:\n",
    "    # cheap probe to get the dimension; for text-embedding-3-small it's 1536\n",
    "    return len(embeddings.embed_query(\"dim?\"))\n",
    "\n",
    "def _ensure_collection(client: QdrantClient, name: str, embeddings) -> None:\n",
    "    if not client.collection_exists(name):\n",
    "        dim = _embedding_dim(embeddings)\n",
    "        client.create_collection(\n",
    "            collection_name=name,\n",
    "            vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "# --- Step 4: Embed and upload to Qdrant ---\n",
    "def embed_and_store(chunks: list, collection_name: str):\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "    # 1) Make sure the collection exists (create-once)\n",
    "    _ensure_collection(client, collection_name, embeddings)\n",
    "\n",
    "    # 2) Tag chunks so you can manage per-document later\n",
    "    # doc_id = str(uuid.uuid4())\n",
    "    # for i, d in enumerate(chunks):\n",
    "    #     d.metadata.setdefault(\"document_id\", doc_id)\n",
    "    #     d.metadata.setdefault(\"chunk_index\", i)\n",
    "\n",
    "    # 3) Reuse the same wrapper and just append\n",
    "    vs = Qdrant(client=client, collection_name=collection_name, embeddings=embeddings)\n",
    "    vs.add_documents(chunks)\n",
    "\n",
    "    print(f\" Added {len(chunks)} chunks to '{collection_name}'\")\n",
    "\n",
    "txt_paths = find_txt_files(ROOT_DIR)\n",
    "raw_docs = load_documents(txt_paths, ROOT_DIR)\n",
    "chunks = chunk_documents(raw_docs)\n",
    "embed_and_store(chunks, COLLECTION_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import uuid\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Qdrant\n",
    "# from qdrant_client import QdrantClient\n",
    "# from langchain.document_loaders import TextLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# COLLECTION_NAME = 'historical_docs'\n",
    "\n",
    "# def ensure_metadata(docs, document_id: str, filename: str):\n",
    "#     for i, d in enumerate(docs):\n",
    "#         d.metadata.setdefault('document_id', document_id)\n",
    "#         d.metadata.setdefault('filename', filename)\n",
    "#         d.metadata.setdefault('chunk_index', i)\n",
    "#     return docs\n",
    "\n",
    "# def upsert_documents(chunks, collection_name=COLLECTION_NAME):\n",
    "#     embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "#     client = QdrantClient(host='localhost', port=6333)\n",
    "\n",
    "#     # Create the collection on first run; otherwise append\n",
    "#     if client.collection_exists(collection_name):\n",
    "#         vs = Qdrant(client=client, collection_name=collection_name, embeddings=embeddings)\n",
    "#         vs.add_documents(chunks)\n",
    "#     else:\n",
    "#         Qdrant.from_documents(\n",
    "#             documents=chunks,\n",
    "#             embedding=embeddings,\n",
    "#             client=client,                 # using the existing client\n",
    "#             collection_name=collection_name,\n",
    "#         )\n",
    "\n",
    "# # Example use per upload:\n",
    "# # 1) load the raw doc -> split into chunks\n",
    "# # 2) add per-doc metadata (document_id, filename)\n",
    "# def index_one_file(path: Path, root_dir: str):\n",
    "#     loader = TextLoader(str(path), encoding='utf-8')\n",
    "#     docs = loader.load()\n",
    "#     splitter = RecursiveCharacterTextSplitter(chunk_size=550, chunk_overlap=50)\n",
    "#     chunks = splitter.split_documents(docs)\n",
    "\n",
    "#     document_id = str(uuid.uuid4())                # stable id for this uploaded file\n",
    "#     filename = str(path.relative_to(root_dir))\n",
    "#     chunks = ensure_metadata(chunks, document_id, filename)\n",
    "\n",
    "#     upsert_documents(chunks, collection_name=COLLECTION_NAME)\n",
    "#     print(f' Added {len(chunks)} chunks from {filename} (document_id={document_id})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
